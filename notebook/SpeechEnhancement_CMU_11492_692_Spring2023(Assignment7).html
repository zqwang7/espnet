<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>CMU 11492/11692 Spring 2023: Speech Enhancement &mdash; ESPnet 202308 documentation</title><link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../_static/nbsphinx-code-cells.css" type="text/css" />
      <link rel="stylesheet" href="../_static/nbsphinx-code-cells.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../_static/jquery.js"></script>
        <script type="text/javascript" src="../_static/underscore.js"></script>
        <script type="text/javascript" src="../_static/doctools.js"></script>
        <script type="text/javascript" src="../_static/language_data.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
        <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true, "ignoreClass": "tex2jax_ignore|mathjax_ignore|document", "processClass": "tex2jax_process|mathjax_process|math|output_area"}})</script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="CMU 11492/11692 Spring 2023: Spoken Language Understanding" href="SpokenLanguageUnderstanding_CMU_11492_692_Spring2023(Assignment6).html" />
    <link rel="prev" title="CMU 11492/11692 Spring 2023: Data preparation" href="DataPreparation_CMU_11492_692_Spring2023(Assignment0).html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            ESPnet
          </a>
              <div class="version">
                202308
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p><span class="caption-text">Tutorial:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorial.html">Common usages</a></li>
<li class="toctree-l1"><a class="reference internal" href="../parallelization.html">Using job scheduling system</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq.html">FAQ</a></li>
<li class="toctree-l1"><a class="reference internal" href="../docker.html">Docker</a></li>
</ul>
<p><span class="caption-text">ESPnet1:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../espnet1_tutorial.html">Usage</a></li>
</ul>
<p><span class="caption-text">ESPnet2:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../espnet2_tutorial.html">ESPnet2</a></li>
<li class="toctree-l1"><a class="reference internal" href="../espnet2_tutorial.html#instruction-for-run-sh">Instruction for run.sh</a></li>
<li class="toctree-l1"><a class="reference internal" href="../espnet2_training_option.html">Change the configuration for training</a></li>
<li class="toctree-l1"><a class="reference internal" href="../espnet2_format_wav_scp.html">Converting audio file formats using format_wav_scp.py</a></li>
<li class="toctree-l1"><a class="reference internal" href="../espnet2_task.html">Task class and data input system for training</a></li>
<li class="toctree-l1"><a class="reference internal" href="../espnet2_distributed.html">Distributed training</a></li>
</ul>
<p><span class="caption-text">Notebook:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="DataPreparation_CMU_11492_692_Spring2023(Assignment0).html">CMU 11492/11692 Spring 2023: Data preparation</a></li>
<li class="toctree-l1"><a class="reference internal" href="DataPreparation_CMU_11492_692_Spring2023(Assignment0).html#Data-preparation-in-ESPnet">Data preparation in ESPnet</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">CMU 11492/11692 Spring 2023: Speech Enhancement</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#❗Important-Notes❗">❗Important Notes❗</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="#Contents">Contents</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#Install">Install</a></li>
<li class="toctree-l2"><a class="reference internal" href="#Speech-Enhancement-with-Pretrained-Models">Speech Enhancement with Pretrained Models</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#Single-Channel-Enhancement,-the-CHiME-example">Single-Channel Enhancement, the CHiME example</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Task1-(✅-Checkpoint-1-(1-point))">Task1 (✅ Checkpoint 1 (1 point))</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#Download-and-load-the-pretrained-Conv-Tasnet">Download and load the pretrained Conv-Tasnet</a></li>
<li class="toctree-l4"><a class="reference internal" href="#Enhance-the-single-channel-real-noisy-speech-in-CHiME4">Enhance the single-channel real noisy speech in CHiME4</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#Multi-Channel-Enhancement">Multi-Channel Enhancement</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#Download-and-load-the-pretrained-mvdr-neural-beamformer.">Download and load the pretrained mvdr neural beamformer.</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#Task2-(✅-Checkpoint-2-(1-point))">Task2 (✅ Checkpoint 2 (1 point))</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#Enhance-the-multi-channel-real-noisy-speech-in-CHiME4">Enhance the multi-channel real noisy speech in CHiME4</a></li>
<li class="toctree-l4"><a class="reference internal" href="#Portable-speech-enhancement-scripts-for-other-tasks">Portable speech enhancement scripts for other tasks</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#Speech-Separation">Speech Separation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#Model-Selection">Model Selection</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Separate-Speech-Mixture">Separate Speech Mixture</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#Separate-the-example-in-wsj0_2mix-testing-set">Separate the example in wsj0_2mix testing set</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#Task3-(✅-Checkpoint-3-(1-point))">Task3 (✅ Checkpoint 3 (1 point))</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#Show-spectrums-of-separated-speech">Show spectrums of separated speech</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#Evaluate-separated-speech-with-pretrained-ASR-model">Evaluate separated speech with pretrained ASR model</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#Task4-(✅-Checkpoint-4-(1-point))">Task4 (✅ Checkpoint 4 (1 point))</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Task5-(✅-Checkpoint-5-(1-point))">Task5 (✅ Checkpoint 5 (1 point))</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="SpokenLanguageUnderstanding_CMU_11492_692_Spring2023(Assignment6).html">CMU 11492/11692 Spring 2023: Spoken Language Understanding</a></li>
<li class="toctree-l1"><a class="reference internal" href="TextToSpeech_CMU_11492_692_Spring2023(Assignment8).html">CMU 11492/11692 Spring 2023: Text to Speech</a></li>
<li class="toctree-l1"><a class="reference internal" href="asr_cli.html">Speech Recognition (Recipe)</a></li>
<li class="toctree-l1"><a class="reference internal" href="asr_library.html">Speech Recognition (Library)</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet2_2pass_slu_demo.html">ESPNET 2 pass SLU Demonstration</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet2_asr_realtime_demo.html">ESPnet2-ASR realtime demonstration</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet2_asr_transfer_learning_demo.html"><strong>Use transfer learning for ASR in ESPnet2</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet2_asr_transfer_learning_demo.html#Abstract">Abstract</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet2_asr_transfer_learning_demo.html#ESPnet-installation-(about-10-minutes-in-total)">ESPnet installation (about 10 minutes in total)</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet2_asr_transfer_learning_demo.html#mini_an4-recipe-as-a-transfer-learning-example">mini_an4 recipe as a transfer learning example</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet2_new_task_tutorial_CMU_11751_18781_Fall2022.html">CMU 11751/18781 Fall 2022: ESPnet Tutorial2 (New task)</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet2_new_task_tutorial_CMU_11751_18781_Fall2022.html#Install-ESPnet-(Almost-same-procedure-as-your-first-tutorial)">Install ESPnet (Almost same procedure as your first tutorial)</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet2_new_task_tutorial_CMU_11751_18781_Fall2022.html#What-we-provide-you-and-what-you-need-to-proceed">What we provide you and what you need to proceed</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet2_recipe_tutorial_CMU_11751_18781_Fall2022.html">CMU 11751/18781 Fall 2022: ESPnet Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet2_recipe_tutorial_CMU_11751_18781_Fall2022.html#Install-ESPnet">Install ESPnet</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet2_recipe_tutorial_CMU_11751_18781_Fall2022.html#Run-an-existing-recipe">Run an existing recipe</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet2_recipe_tutorial_CMU_11751_18781_Fall2022.html#Make-a-new-recipe">Make a new recipe</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet2_recipe_tutorial_CMU_11751_18781_Fall2022.html#Additional-resources">Additional resources</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet2_streaming_asr_demo.html">ESPnet2 real streaming Transformer demonstration</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet2_tts_realtime_demo.html">ESPnet2-TTS realtime demonstration</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet2_tutorial_2021_CMU_11751_18781.html">CMU 11751/18781 2021: ESPnet Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet2_tutorial_2021_CMU_11751_18781.html#Run-an-inference-example">Run an inference example</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet2_tutorial_2021_CMU_11751_18781.html#Full-installation">Full installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet2_tutorial_2021_CMU_11751_18781.html#Run-a-recipe-example">Run a recipe example</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet_se_demonstration_for_waspaa_2021.html">ESPnet Speech Enhancement Demonstration</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet_se_demonstration_for_waspaa_2021.html#Contents">Contents</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet_se_demonstration_for_waspaa_2021.html#(1)-Tutorials-on-the-Basic-Usage">(1) Tutorials on the Basic Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="espnet_se_demonstration_for_waspaa_2021.html#(2)-Tutorials-on-Contributing-to-ESPNet-SE-Project">(2) Tutorials on Contributing to ESPNet-SE Project</a></li>
<li class="toctree-l1"><a class="reference internal" href="onnx_conversion_demo.html">espnet_onnx demonstration</a></li>
<li class="toctree-l1"><a class="reference internal" href="onnx_conversion_demo.html#Install-Dependency">Install Dependency</a></li>
<li class="toctree-l1"><a class="reference internal" href="onnx_conversion_demo.html#Export-your-model">Export your model</a></li>
<li class="toctree-l1"><a class="reference internal" href="onnx_conversion_demo.html#Inference-with-onnx">Inference with onnx</a></li>
<li class="toctree-l1"><a class="reference internal" href="onnx_conversion_demo.html#Using-streaming-model">Using streaming model</a></li>
<li class="toctree-l1"><a class="reference internal" href="pretrained.html">Pretrained Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="se_demo.html">ESPnet Speech Enhancement Demonstration</a></li>
<li class="toctree-l1"><a class="reference internal" href="st_demo.html">ESPnet Speech Translation Demonstration</a></li>
<li class="toctree-l1"><a class="reference internal" href="tts_cli.html">Text-to-Speech (Recipe)</a></li>
<li class="toctree-l1"><a class="reference internal" href="tts_realtime_demo.html">ESPnet real time E2E-TTS demonstration</a></li>
</ul>
<p><span class="caption-text">Package Reference:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../_gen/espnet.distributed.html">espnet.distributed package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../_gen/espnet.tts.html">espnet.tts package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../_gen/espnet.asr.html">espnet.asr package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../_gen/espnet.st.html">espnet.st package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../_gen/espnet.transform.html">espnet.transform package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../_gen/espnet.bin.html">espnet.bin package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../_gen/espnet.optimizer.html">espnet.optimizer package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../_gen/espnet.nets.html">espnet.nets package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../_gen/espnet.scheduler.html">espnet.scheduler package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../_gen/espnet.vc.html">espnet.vc package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../_gen/espnet.mt.html">espnet.mt package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../_gen/espnet.lm.html">espnet.lm package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../_gen/espnet.utils.html">espnet.utils package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../_gen/espnet2.svs.html">espnet2.svs package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../_gen/espnet2.tts.html">espnet2.tts package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../_gen/espnet2.asr.html">espnet2.asr package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../_gen/espnet2.asr_transducer.html">espnet2.asr_transducer package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../_gen/espnet2.st.html">espnet2.st package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../_gen/espnet2.bin.html">espnet2.bin package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../_gen/espnet2.asvspoof.html">espnet2.asvspoof package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../_gen/espnet2.schedulers.html">espnet2.schedulers package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../_gen/espnet2.gan_svs.html">espnet2.gan_svs package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../_gen/espnet2.fileio.html">espnet2.fileio package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../_gen/espnet2.slu.html">espnet2.slu package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../_gen/espnet2.hubert.html">espnet2.hubert package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../_gen/espnet2.gan_tts.html">espnet2.gan_tts package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../_gen/espnet2.optimizers.html">espnet2.optimizers package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../_gen/espnet2.uasr.html">espnet2.uasr package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../_gen/espnet2.spk.html">espnet2.spk package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../_gen/espnet2.enh.html">espnet2.enh package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../_gen/espnet2.train.html">espnet2.train package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../_gen/espnet2.main_funcs.html">espnet2.main_funcs package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../_gen/espnet2.samplers.html">espnet2.samplers package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../_gen/espnet2.mt.html">espnet2.mt package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../_gen/espnet2.fst.html">espnet2.fst package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../_gen/espnet2.lm.html">espnet2.lm package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../_gen/espnet2.layers.html">espnet2.layers package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../_gen/espnet2.utils.html">espnet2.utils package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../_gen/espnet2.tasks.html">espnet2.tasks package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../_gen/espnet2.iterators.html">espnet2.iterators package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../_gen/espnet2.text.html">espnet2.text package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../_gen/espnet2.torch_utils.html">espnet2.torch_utils package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../_gen/espnet2.diar.html">espnet2.diar package</a></li>
</ul>
<p><span class="caption-text">Tool Reference:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../apis/espnet_bin.html">core tools</a></li>
<li class="toctree-l1"><a class="reference internal" href="../apis/espnet2_bin.html">core tools (espnet2)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../apis/utils_py.html">python utility tools</a></li>
<li class="toctree-l1"><a class="reference internal" href="../apis/utils_sh.html">bash utility tools</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">ESPnet</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">CMU 11492/11692 Spring 2023: Speech Enhancement</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/notebook/SpeechEnhancement_CMU_11492_692_Spring2023(Assignment7).ipynb.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="CMU-11492/11692-Spring-2023:-Speech-Enhancement">
<h1>CMU 11492/11692 Spring 2023: Speech Enhancement<a class="headerlink" href="#CMU-11492/11692-Spring-2023:-Speech-Enhancement" title="Permalink to this headline">¶</a></h1>
<p>In this demonstration, we will show you some demonstrations of speech enhancement systems in ESPnet.</p>
<p>Main references: - <a class="reference external" href="https://github.com/espnet/espnet">ESPnet repository</a> - <a class="reference external" href="https://espnet.github.io/espnet/">ESPnet documentation</a> - <a class="reference external" href="https://github.com/espnet/espnet/tree/master/egs2/TEMPLATE/enh1">ESPnet-SE repo</a></p>
<p>Author: - Siddhant Arora (<a class="reference external" href="mailto:siddhana&#37;&#52;&#48;andrew&#46;cmu&#46;edu">siddhana<span>&#64;</span>andrew<span>&#46;</span>cmu<span>&#46;</span>edu</a>)</p>
<p>The notebook is adapted from this <a class="reference external" href="https://colab.research.google.com/drive/1faFfqWNFe1QW3Q1PMwRXlNDwaBms__Ho?usp=sharing">Colab</a></p>
<section id="❗Important-Notes❗">
<h2>❗Important Notes❗<a class="headerlink" href="#❗Important-Notes❗" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>We are using Colab to show the demo. However, Colab has some constraints on the total GPU runtime. If you use too much GPU time, you may not be able to use GPU for some time.</p></li>
<li><p>There are multiple in-class checkpoints ✅ throughout this tutorial. <strong>Your participation points are based on these tasks.</strong> Please try your best to follow all the steps! If you encounter issues, please notify the TAs as soon as possible so that we can make an adjustment for you.</p></li>
<li><p>Please submit PDF files of your completed notebooks to Gradescope. You can print the notebook using <code class="docutils literal notranslate"><span class="pre">File</span> <span class="pre">-&gt;</span> <span class="pre">Print</span></code> in the menu bar.You also need to submit the spectrogram and waveform of noisy and enhanced audio files to Gradescope.</p></li>
</ul>
</section>
</section>
<section id="Contents">
<h1>Contents<a class="headerlink" href="#Contents" title="Permalink to this headline">¶</a></h1>
<p>Tutorials on the Basic Usage</p>
<ol class="arabic simple">
<li><p>Install</p></li>
<li><p>Speech Enhancement with Pretrained Models</p></li>
</ol>
<blockquote>
<div><p>We support various interfaces, e.g. Python API, HuggingFace API, portable speech enhancement scripts for other tasks, etc.</p>
</div></blockquote>
<p>2.1 Single-channel Enhancement (CHiME-4)</p>
<p>2.2 Enhance Your Own Recordings</p>
<p>2.3 Multi-channel Enhancement (CHiME-4)</p>
<ol class="arabic simple" start="3">
<li><p>Speech Separation with Pretrained Models</p></li>
</ol>
<p>3.1 Model Selection</p>
<p>3.2 Separate Speech Mixture</p>
<ol class="arabic simple" start="4">
<li><p>Evaluate Separated Speech with the Pretrained ASR Model</p></li>
</ol>
<p>Tutorials on the Basic Usage</p>
<section id="Install">
<h2>Install<a class="headerlink" href="#Install" title="Permalink to this headline">¶</a></h2>
<p>Different from previous assignment where we install the full version of ESPnet, we use a lightweight ESPnet package, which mainly designed for inference purpose. The installation with the light version can be much faster than a full installation.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre><span></span>import locale
locale.getpreferredencoding = lambda: &quot;UTF-8&quot;
%pip uninstall torch
%pip install torch==1.13.0+cu117 torchvision==0.14.0+cu117 torchaudio==0.13.0 --extra-index-url https://download.pytorch.org/whl/cu117
%pip install -q git+https://github.com/espnet/espnet
%pip install -q espnet_model_zoo
</pre></div>
</div>
</div>
</section>
<section id="Speech-Enhancement-with-Pretrained-Models">
<h2>Speech Enhancement with Pretrained Models<a class="headerlink" href="#Speech-Enhancement-with-Pretrained-Models" title="Permalink to this headline">¶</a></h2>
<section id="Single-Channel-Enhancement,-the-CHiME-example">
<h3>Single-Channel Enhancement, the CHiME example<a class="headerlink" href="#Single-Channel-Enhancement,-the-CHiME-example" title="Permalink to this headline">¶</a></h3>
</section>
<section id="Task1-(✅-Checkpoint-1-(1-point))">
<h3>Task1 (✅ Checkpoint 1 (1 point))<a class="headerlink" href="#Task1-(✅-Checkpoint-1-(1-point))" title="Permalink to this headline">¶</a></h3>
<p>Run inference of pretrained single-channel enhancement model.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre><span></span># Download one utterance from real noisy speech of CHiME4
!gdown --id 1SmrN5NFSg6JuQSs2sfy3ehD8OIcqK6wS -O /content/M05_440C0213_PED_REAL.wav
import os

import soundfile
from IPython.display import display, Audio
mixwav_mc, sr = soundfile.read(&quot;/content/M05_440C0213_PED_REAL.wav&quot;)
# mixwav.shape: num_samples, num_channels
mixwav_sc = mixwav_mc[:,4]
display(Audio(mixwav_mc.T, rate=sr))
</pre></div>
</div>
</div>
<section id="Download-and-load-the-pretrained-Conv-Tasnet">
<h4>Download and load the pretrained Conv-Tasnet<a class="headerlink" href="#Download-and-load-the-pretrained-Conv-Tasnet" title="Permalink to this headline">¶</a></h4>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre><span></span>!gdown --id 17DMWdw84wF3fz3t7ia1zssdzhkpVQGZm -O /content/chime_tasnet_singlechannel.zip
!unzip /content/chime_tasnet_singlechannel.zip -d /content/enh_model_sc
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre><span></span># Load the model
# If you encounter error &quot;No module named &#39;espnet2&#39;&quot;, please re-run the 1st Cell. This might be a colab bug.
import sys
import soundfile
from espnet2.bin.enh_inference import SeparateSpeech


separate_speech = {}
# For models downloaded from GoogleDrive, you can use the following script:
enh_model_sc = SeparateSpeech(
  train_config=&quot;/content/enh_model_sc/exp/enh_train_enh_conv_tasnet_raw/config.yaml&quot;,
  model_file=&quot;/content/enh_model_sc/exp/enh_train_enh_conv_tasnet_raw/5epoch.pth&quot;,
  # for segment-wise process on long speech
  normalize_segment_scale=False,
  show_progressbar=True,
  ref_channel=4,
  normalize_output_wav=True,
  device=&quot;cuda:0&quot;,
)
</pre></div>
</div>
</div>
</section>
<section id="Enhance-the-single-channel-real-noisy-speech-in-CHiME4">
<h4>Enhance the single-channel real noisy speech in CHiME4<a class="headerlink" href="#Enhance-the-single-channel-real-noisy-speech-in-CHiME4" title="Permalink to this headline">¶</a></h4>
<p>Please submit the screenshot of output of current block and the spectogram and waveform of noisy and enhanced speech file to Gradescope for Task 1.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre><span></span># play the enhanced single-channel speech
wave = enh_model_sc(mixwav_sc[None, ...], sr)

print(&quot;Input real noisy speech&quot;, flush=True)
display(Audio(mixwav_sc, rate=sr))
print(&quot;Enhanced speech&quot;, flush=True)
display(Audio(wave[0].squeeze(), rate=sr))
</pre></div>
</div>
</div>
</section>
</section>
<section id="Multi-Channel-Enhancement">
<h3>Multi-Channel Enhancement<a class="headerlink" href="#Multi-Channel-Enhancement" title="Permalink to this headline">¶</a></h3>
<section id="Download-and-load-the-pretrained-mvdr-neural-beamformer.">
<h4>Download and load the pretrained mvdr neural beamformer.<a class="headerlink" href="#Download-and-load-the-pretrained-mvdr-neural-beamformer." title="Permalink to this headline">¶</a></h4>
</section>
</section>
<section id="Task2-(✅-Checkpoint-2-(1-point))">
<h3>Task2 (✅ Checkpoint 2 (1 point))<a class="headerlink" href="#Task2-(✅-Checkpoint-2-(1-point))" title="Permalink to this headline">¶</a></h3>
<p>Run inference of pretrained multi-channel enhancement model.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre><span></span># Download the pretained enhancement model

!gdown --id 1FohDfBlOa7ipc9v2luY-QIFQ_GJ1iW_i -O /content/mvdr_beamformer_16k_se_raw_valid.zip
!unzip /content/mvdr_beamformer_16k_se_raw_valid.zip -d /content/enh_model_mc
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre><span></span># Load the model
# If you encounter error &quot;No module named &#39;espnet2&#39;&quot;, please re-run the 1st Cell. This might be a colab bug.
import sys
import soundfile
from espnet2.bin.enh_inference import SeparateSpeech


separate_speech = {}
# For models downloaded from GoogleDrive, you can use the following script:
enh_model_mc = SeparateSpeech(
  train_config=&quot;/content/enh_model_mc/exp/enh_train_enh_beamformer_mvdr_raw/config.yaml&quot;,
  model_file=&quot;/content/enh_model_mc/exp/enh_train_enh_beamformer_mvdr_raw/11epoch.pth&quot;,
  # for segment-wise process on long speech
  normalize_segment_scale=False,
  show_progressbar=True,
  ref_channel=4,
  normalize_output_wav=True,
  device=&quot;cuda:0&quot;,
)
</pre></div>
</div>
</div>
<section id="Enhance-the-multi-channel-real-noisy-speech-in-CHiME4">
<h4>Enhance the multi-channel real noisy speech in CHiME4<a class="headerlink" href="#Enhance-the-multi-channel-real-noisy-speech-in-CHiME4" title="Permalink to this headline">¶</a></h4>
<p>Please submit the screenshot of output of current block and the spectrogram and waveform of noisy and enhanced speech file to Gradescope for Task 2.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre><span></span>wave = enh_model_mc(mixwav_mc[None, ...], sr)
print(&quot;Input real noisy speech&quot;, flush=True)
display(Audio(mixwav_mc.T, rate=sr))
print(&quot;Enhanced speech&quot;, flush=True)
display(Audio(wave[0].squeeze(), rate=sr))
</pre></div>
</div>
</div>
</section>
<section id="Portable-speech-enhancement-scripts-for-other-tasks">
<h4>Portable speech enhancement scripts for other tasks<a class="headerlink" href="#Portable-speech-enhancement-scripts-for-other-tasks" title="Permalink to this headline">¶</a></h4>
<p>For an ESPNet ASR or TTS dataset like below:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>data
`-- et05_real_isolated_6ch_track
    |-- spk2utt
    |-- text
    |-- utt2spk
    |-- utt2uniq
    `-- wav.scp
</pre></div>
</div>
<p>Run the following scripts to create an enhanced dataset:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>scripts/utils/enhance_dataset.sh \
    --spk_num 1 \
    --gpu_inference true \
    --inference_nj 4 \
    --fs 16k \
    --id_prefix &quot;&quot; \
    dump/raw/et05_real_isolated_6ch_track \
    data/et05_real_isolated_6ch_track_enh \
    exp/enh_train_enh_beamformer_mvdr_raw/valid.loss.best.pth
</pre></div>
</div>
<p>The above script will generate a new directory data/et05_real_isolated_6ch_track_enh:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>data
`-- et05_real_isolated_6ch_track_enh
    |-- spk2utt
    |-- text
    |-- utt2spk
    |-- utt2uniq
    |-- wav.scp
    `-- wavs/
</pre></div>
</div>
<p>where wav.scp contains paths to the enhanced audios (stored in wavs/).</p>
</section>
</section>
</section>
<section id="Speech-Separation">
<h2>Speech Separation<a class="headerlink" href="#Speech-Separation" title="Permalink to this headline">¶</a></h2>
<section id="Model-Selection">
<h3>Model Selection<a class="headerlink" href="#Model-Selection" title="Permalink to this headline">¶</a></h3>
<p>In this demonstration, we will show different speech separation models on wsj0_2mix.</p>
<p>The pretrained models can be download from a direct URL, or from <a class="reference external" href="https://zenodo.org/">zenodo</a> and <a class="reference external" href="https://huggingface.co/">huggingface</a> with the corresponding model ID.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre><span></span>!gdown --id 1TasZxZSnbSPsk_Wf7ZDhBAigS6zN8G9G -O enh_train_enh_tfgridnet_tf_lr-patience3_patience5_raw_valid.loss.ave.zip
!unzip enh_train_enh_tfgridnet_tf_lr-patience3_patience5_raw_valid.loss.ave.zip -d /content/enh_model_ss
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre><span></span>import sys
import soundfile
from espnet2.bin.enh_inference import SeparateSpeech

# For models downloaded from GoogleDrive, you can use the following script:
separate_speech = SeparateSpeech(
  train_config=&quot;/content/enh_model_ss/exp/enh_train_enh_tfgridnet_tf_lr-patience3_patience5_raw/config.yaml&quot;,
  model_file=&quot;/content/enh_model_ss/exp/enh_train_enh_tfgridnet_tf_lr-patience3_patience5_raw/98epoch.pth&quot;,
  # for segment-wise process on long speech
  segment_size=2.4,
  hop_size=0.8,
  normalize_segment_scale=False,
  show_progressbar=True,
  ref_channel=None,
  normalize_output_wav=True,
  device=&quot;cuda:0&quot;,
)
</pre></div>
</div>
</div>
</section>
<section id="Separate-Speech-Mixture">
<h3>Separate Speech Mixture<a class="headerlink" href="#Separate-Speech-Mixture" title="Permalink to this headline">¶</a></h3>
<section id="Separate-the-example-in-wsj0_2mix-testing-set">
<h4>Separate the example in wsj0_2mix testing set<a class="headerlink" href="#Separate-the-example-in-wsj0_2mix-testing-set" title="Permalink to this headline">¶</a></h4>
</section>
</section>
<section id="Task3-(✅-Checkpoint-3-(1-point))">
<h3>Task3 (✅ Checkpoint 3 (1 point))<a class="headerlink" href="#Task3-(✅-Checkpoint-3-(1-point))" title="Permalink to this headline">¶</a></h3>
<p>Run inference of pretrained speech seperation model based on TF-GRIDNET.</p>
<p>Please submit the screenshot of output of current block and the spectrogram and waveform of mixed and seperated speech files to Gradescope for Task 3.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre><span></span>!gdown --id 1ZCUkd_Lb7pO2rpPr4FqYdtJBZ7JMiInx -O /content/447c020t_1.2106_422a0112_-1.2106.wav

import os
import soundfile
from IPython.display import display, Audio

mixwav, sr = soundfile.read(&quot;447c020t_1.2106_422a0112_-1.2106.wav&quot;)
waves_wsj = separate_speech(mixwav[None, ...], fs=sr)

print(&quot;Input mixture&quot;, flush=True)
display(Audio(mixwav, rate=sr))
print(f&quot;========= Separated speech with model =========&quot;, flush=True)
print(&quot;Separated spk1&quot;, flush=True)
display(Audio(waves_wsj[0].squeeze(), rate=sr))
print(&quot;Separated spk2&quot;, flush=True)
display(Audio(waves_wsj[1].squeeze(), rate=sr))
</pre></div>
</div>
</div>
<section id="Show-spectrums-of-separated-speech">
<h4>Show spectrums of separated speech<a class="headerlink" href="#Show-spectrums-of-separated-speech" title="Permalink to this headline">¶</a></h4>
<p>Show wavform and spectrogram of mixed and seperated speech.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre><span></span>import matplotlib.pyplot as plt
import torch
from torch_complex.tensor import ComplexTensor

from espnet.asr.asr_utils import plot_spectrogram
from espnet2.layers.stft import Stft


stft = Stft(
  n_fft=512,
  win_length=None,
  hop_length=128,
  window=&quot;hann&quot;,
)
ilens = torch.LongTensor([len(mixwav)])
# specs: (T, F)
spec_mix = ComplexTensor(
    *torch.unbind(
      stft(torch.as_tensor(mixwav).unsqueeze(0), ilens)[0].squeeze(),
      dim=-1
  )
)
spec_sep1 = ComplexTensor(
    *torch.unbind(
      stft(torch.as_tensor(waves_wsj[0]), ilens)[0].squeeze(),
      dim=-1
  )
)
spec_sep2 = ComplexTensor(
    *torch.unbind(
      stft(torch.as_tensor(waves_wsj[1]), ilens)[0].squeeze(),
      dim=-1
  )
)

samples = torch.linspace(0, len(mixwav) / sr, len(mixwav))
plt.figure(figsize=(24, 12))
plt.subplot(3, 2, 1)
plt.title(&#39;Mixture Spectrogram&#39;)
plot_spectrogram(
  plt, abs(spec_mix).transpose(-1, -2).numpy(), fs=sr,
  mode=&#39;db&#39;, frame_shift=None,
  bottom=False, labelbottom=False
)
plt.subplot(3, 2, 2)
plt.title(&#39;Mixture Wavform&#39;)
plt.plot(samples, mixwav)
plt.xlim(0, len(mixwav) / sr)

plt.subplot(3, 2, 3)
plt.title(&#39;Separated Spectrogram (spk1)&#39;)
plot_spectrogram(
  plt, abs(spec_sep1).transpose(-1, -2).numpy(), fs=sr,
  mode=&#39;db&#39;, frame_shift=None,
  bottom=False, labelbottom=False
)
plt.subplot(3, 2, 4)
plt.title(&#39;Separated Wavform (spk1)&#39;)
plt.plot(samples, waves_wsj[0].squeeze())
plt.xlim(0, len(mixwav) / sr)

plt.subplot(3, 2, 5)
plt.title(&#39;Separated Spectrogram (spk2)&#39;)
plot_spectrogram(
  plt, abs(spec_sep2).transpose(-1, -2).numpy(), fs=sr,
  mode=&#39;db&#39;, frame_shift=None,
  bottom=False, labelbottom=False
)
plt.subplot(3, 2, 6)
plt.title(&#39;Separated Wavform (spk2)&#39;)
plt.plot(samples, waves_wsj[1].squeeze())
plt.xlim(0, len(mixwav) / sr)
plt.xlabel(&quot;Time (s)&quot;)
plt.show()
</pre></div>
</div>
</div>
</section>
</section>
</section>
<section id="Evaluate-separated-speech-with-pretrained-ASR-model">
<h2>Evaluate separated speech with pretrained ASR model<a class="headerlink" href="#Evaluate-separated-speech-with-pretrained-ASR-model" title="Permalink to this headline">¶</a></h2>
<p>The ground truths are:</p>
<p><code class="docutils literal notranslate"><span class="pre">text_1:</span> <span class="pre">SOME</span> <span class="pre">CRITICS</span> <span class="pre">INCLUDING</span> <span class="pre">HIGH</span> <span class="pre">REAGAN</span> <span class="pre">ADMINISTRATION</span> <span class="pre">OFFICIALS</span> <span class="pre">ARE</span> <span class="pre">RAISING</span> <span class="pre">THE</span> <span class="pre">ALARM</span> <span class="pre">THAT</span> <span class="pre">THE</span> <span class="pre">FED'S</span> <span class="pre">POLICY</span> <span class="pre">IS</span> <span class="pre">TOO</span> <span class="pre">TIGHT</span> <span class="pre">AND</span> <span class="pre">COULD</span> <span class="pre">CAUSE</span> <span class="pre">A</span> <span class="pre">RECESSION</span> <span class="pre">NEXT</span> <span class="pre">YEAR</span></code></p>
<p><code class="docutils literal notranslate"><span class="pre">text_2:</span> <span class="pre">THE</span> <span class="pre">UNITED</span> <span class="pre">STATES</span> <span class="pre">UNDERTOOK</span> <span class="pre">TO</span> <span class="pre">DEFEND</span> <span class="pre">WESTERN</span> <span class="pre">EUROPE</span> <span class="pre">AGAINST</span> <span class="pre">SOVIET</span> <span class="pre">ATTACK</span></code></p>
<p>(This may take a while for the speech recognition.)</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre><span></span>%pip install -q https://github.com/kpu/kenlm/archive/master.zip # ASR needs kenlm
</pre></div>
</div>
</div>
<section id="Task4-(✅-Checkpoint-4-(1-point))">
<h3>Task4 (✅ Checkpoint 4 (1 point))<a class="headerlink" href="#Task4-(✅-Checkpoint-4-(1-point))" title="Permalink to this headline">¶</a></h3>
<p>Show inference of pre-trained ASR model on mixed and seperated speech.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre><span></span>!gdown --id 1H7--jXTTwmwxzfO8LT5kjZyBjng-HxED -O asr_train_asr_transformer_raw_char_1gpu_valid.acc.ave.zip
!unzip asr_train_asr_transformer_raw_char_1gpu_valid.acc.ave.zip -d /content/asr_model
!ln -sf /content/asr_model/exp .
</pre></div>
</div>
</div>
<p>Please submit the screenshot of ASR inference on Mix Speech and Separated Speech 1 and Separated Speech 2 files to Gradescope for Task 4.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre><span></span>import espnet_model_zoo
from espnet2.bin.asr_inference import Speech2Text


# For models downloaded from GoogleDrive, you can use the following script:
speech2text = Speech2Text(
  asr_train_config=&quot;/content/asr_model/exp/asr_train_asr_transformer_raw_char_1gpu/config.yaml&quot;,
  asr_model_file=&quot;/content/asr_model/exp/asr_train_asr_transformer_raw_char_1gpu/valid.acc.ave_10best.pth&quot;,
  device=&quot;cuda:0&quot;
)

text_est = [None, None]
text_est[0], *_ = speech2text(waves_wsj[0].squeeze())[0]
text_est[1], *_ = speech2text(waves_wsj[1].squeeze())[0]
text_m, *_ = speech2text(mixwav)[0]
print(&quot;Mix Speech to Text: &quot;, text_m)
print(&quot;Separated Speech 1 to Text: &quot;, text_est[0])
print(&quot;Separated Speech 2 to Text: &quot;, text_est[1])
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre><span></span>import difflib
from itertools import permutations

import editdistance
import numpy as np

colors = dict(
    red=lambda text: f&quot;\033[38;2;255;0;0m{text}\033[0m&quot; if text else &quot;&quot;,
    green=lambda text: f&quot;\033[38;2;0;255;0m{text}\033[0m&quot; if text else &quot;&quot;,
    yellow=lambda text: f&quot;\033[38;2;225;225;0m{text}\033[0m&quot; if text else &quot;&quot;,
    white=lambda text: f&quot;\033[38;2;255;255;255m{text}\033[0m&quot; if text else &quot;&quot;,
    black=lambda text: f&quot;\033[38;2;0;0;0m{text}\033[0m&quot; if text else &quot;&quot;,
)

def diff_strings(ref, est):
    &quot;&quot;&quot;Reference: https://stackoverflow.com/a/64404008/7384873&quot;&quot;&quot;
    ref_str, est_str, err_str = [], [], []
    matcher = difflib.SequenceMatcher(None, ref, est)
    for opcode, a0, a1, b0, b1 in matcher.get_opcodes():
        if opcode == &quot;equal&quot;:
            txt = ref[a0:a1]
            ref_str.append(txt)
            est_str.append(txt)
            err_str.append(&quot; &quot; * (a1 - a0))
        elif opcode == &quot;insert&quot;:
            ref_str.append(&quot;*&quot; * (b1 - b0))
            est_str.append(colors[&quot;green&quot;](est[b0:b1]))
            err_str.append(colors[&quot;black&quot;](&quot;I&quot; * (b1 - b0)))
        elif opcode == &quot;delete&quot;:
            ref_str.append(ref[a0:a1])
            est_str.append(colors[&quot;red&quot;](&quot;*&quot; * (a1 - a0)))
            err_str.append(colors[&quot;black&quot;](&quot;D&quot; * (a1 - a0)))
        elif opcode == &quot;replace&quot;:
            diff = a1 - a0 - b1 + b0
            if diff &gt;= 0:
                txt_ref = ref[a0:a1]
                txt_est = colors[&quot;yellow&quot;](est[b0:b1]) + colors[&quot;red&quot;](&quot;*&quot; * diff)
                txt_err = &quot;S&quot; * (b1 - b0) + &quot;D&quot; * diff
            elif diff &lt; 0:
                txt_ref = ref[a0:a1] + &quot;*&quot; * -diff
                txt_est = colors[&quot;yellow&quot;](est[b0:b1]) + colors[&quot;green&quot;](&quot;*&quot; * -diff)
                txt_err = &quot;S&quot; * (b1 - b0) + &quot;I&quot; * -diff

            ref_str.append(txt_ref)
            est_str.append(txt_est)
            err_str.append(colors[&quot;black&quot;](txt_err))
    return &quot;&quot;.join(ref_str), &quot;&quot;.join(est_str), &quot;&quot;.join(err_str)


text_ref = [
  &quot;SOME CRITICS INCLUDING HIGH REAGAN ADMINISTRATION OFFICIALS ARE RAISING THE ALARM THAT THE FED&#39;S POLICY IS TOO TIGHT AND COULD CAUSE A RECESSION NEXT YEAR&quot;,
  &quot;THE UNITED STATES UNDERTOOK TO DEFEND WESTERN EUROPE AGAINST SOVIET ATTACK&quot;,
]

print(&quot;=====================&quot; , flush=True)
perms = list(permutations(range(2)))
string_edit = [
  [
    editdistance.eval(text_ref[m], text_est[n])
    for m, n in enumerate(p)
  ]
  for p in perms
]

dist = [sum(edist) for edist in string_edit]
perm_idx = np.argmin(dist)
perm = perms[perm_idx]

for i, p in enumerate(perm):
  print(&quot;\n--------------- Text %d ---------------&quot; % (i + 1), flush=True)
  ref, est, err = diff_strings(text_ref[i], text_est[p])
  print(&quot;REF: &quot; + ref + &quot;\n&quot; + &quot;HYP: &quot; + est + &quot;\n&quot; + &quot;ERR: &quot; + err, flush=True)
  print(&quot;Edit Distance = {}\n&quot;.format(string_edit[perm_idx][i]), flush=True)
</pre></div>
</div>
</div>
</section>
<section id="Task5-(✅-Checkpoint-5-(1-point))">
<h3>Task5 (✅ Checkpoint 5 (1 point))<a class="headerlink" href="#Task5-(✅-Checkpoint-5-(1-point))" title="Permalink to this headline">¶</a></h3>
<p>Enhance your own pre-recordings. Your input speech can be recorded by yourself or you can also find it from other sources (e.g., youtube).</p>
<p>Discuss whether input speech was clearly denoised, and if not, what would be a potential reason.</p>
<p>[YOUR ANSWER HERE]</p>
<p>Please submit the spectrogram and waveform of your input and enhanced speech to GradeScope for Task 5 along with the screenshot of your answer.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre><span></span>from google.colab import files
from IPython.display import display, Audio
import soundfile
fs = 16000
uploaded = files.upload()

for file_name in uploaded.keys():
  speech, rate = soundfile.read(file_name)
  assert rate == fs, &quot;mismatch in sampling rate&quot;
  wave = enh_model_sc(speech[None, ...], fs)
  print(f&quot;Your input speech {file_name}&quot;, flush=True)
  display(Audio(speech, rate=fs))
  print(f&quot;Enhanced speech for {file_name}&quot;, flush=True)
  display(Audio(wave[0].squeeze(), rate=fs))
</pre></div>
</div>
</div>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="DataPreparation_CMU_11492_692_Spring2023(Assignment0).html" class="btn btn-neutral float-left" title="CMU 11492/11692 Spring 2023: Data preparation" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="SpokenLanguageUnderstanding_CMU_11492_692_Spring2023(Assignment6).html" class="btn btn-neutral float-right" title="CMU 11492/11692 Spring 2023: Spoken Language Understanding" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2017, Shinji Watanabe.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>